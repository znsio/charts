{{- if .Values.enabled }}
{{- if .Values.istioMetrics.enabled }}
apiVersion: sloth.slok.dev/v1
kind: PrometheusServiceLevel
metadata:
  labels:
    release: {{ .Values.prometheusReleaseLabel }}
  name: {{- if .Values.includeReleaseNameInMetricsLabels }} {{ .Release.Name }}-{{ .Values.appLabel }}{{- else }} {{ .Values.appLabel }}{{- end }}-availability-slo
  namespace: {{ .Values.namespace }}
spec:
  labels:
    app: sloth
    role: alert-rules
    component: {{- if .Values.includeReleaseNameInMetricsLabels }} {{ .Release.Name }}-{{ .Values.appLabel }}{{- else }} {{ .Values.appLabel }}{{- end }}-availability-SLO-rules
  service: {{- if .Values.includeReleaseNameInMetricsLabels }} {{ .Release.Name }}-{{ .Values.appLabel }}{{- else }} {{ .Values.appLabel }}{{- end }}
  slos:
  - alerting:
      annotations:
        {{- if .Values.grafanaURL }}
        dashboard: {{ .Values.grafanaURL }}/d/slo-detail?var-service={{- if .Values.includeReleaseNameInMetricsLabels }}{{ .Release.Name }}-{{ .Values.appLabel }}{{- else }}{{ .Values.appLabel }}{{- end }}
        {{- end }}
        summary: SLO to measure success vs errors - {{ .Values.availabilitySLO }}% of the time requests should
          be succesfully served (non 5xx). When you receive this alert it means that
          the SLO is at risk as your error budget is getting exhausted. To know more
          about ErrorBudgets and SLOs read https://sre.google/workbook/implementing-slos/
        description: SLO to measure success vs errors - {{ .Values.availabilitySLO }}% of the time requests should
          be succesfully served (non 5xx). When you receive this alert it means that the
          SLO is at risk as your error budget is getting exhausted. To know more about
          ErrorBudgets and SLOs read https://sre.google/workbook/implementing-slos/
      name: {{- if .Values.includeReleaseNameInMetricsLabels }} {{ .Release.Name }}-{{ .Values.appLabel }}{{- else }} {{ .Values.appLabel }}{{- end }} - availability SLO is at RISK 
      pageAlert:
        labels:
          alert_type: symptom
          severity: critical
      ticketAlert:
        labels:
          alert_type: symptom
          severity: warning
    description: SLO to measure success vs errors - {{ .Values.availabilitySLO }}% of the time requests should
      be succesfully served (non 5xx). When you receive this alert it means that the
      SLO is at risk as your error budget is getting exhausted. To know more about
      ErrorBudgets and SLOs read https://sre.google/workbook/implementing-slos/
    name: availability-{{- if .Values.includeReleaseNameInMetricsLabels }}{{ .Release.Name }}-{{ .Values.appLabel }}{{- else }}{{ .Values.appLabel }}{{- end }}
    objective: {{ .Values.availabilitySLO }}
    sli:
      events:
        {{- if .Values.includeReleaseNameInMetricsLabels }}
        errorQuery: sum(rate(istio_requests_total{app="{{ .Release.Name }}-{{ .Values.appLabel}}", destination_app=~"{{ .Release.Name }}-{{ .Values.appLabel}}", response_code=~"5.."}[{{ printf "{{.window}}" }}]))
        totalQuery: sum(rate(istio_requests_total{app="{{ .Release.Name }}-{{ .Values.appLabel}}", destination_app=~"{{ .Release.Name }}-{{ .Values.appLabel}}"}[{{ printf "{{.window}}" }}]))
        {{- else }}
        errorQuery: sum(rate(istio_requests_total{app="{{ .Values.appLabel }}", destination_app=~"{{ .Values.appLabel }}", response_code=~"5.."}[{{ printf "{{.window}}" }}]))
        totalQuery: sum(rate(istio_requests_total{app="{{ .Values.appLabel }}", destination_app=~"{{ .Values.appLabel }}"}[{{ printf "{{.window}}" }}]))
        {{- end }}
---
apiVersion: sloth.slok.dev/v1
kind: PrometheusServiceLevel
metadata:
  labels:
    release: {{ .Values.prometheusReleaseLabel }}
  name: {{- if .Values.includeReleaseNameInMetricsLabels }} {{ .Release.Name }}-{{ .Values.appLabel }}{{- else }} {{ .Values.appLabel }}{{- end }}-latency-slo
  namespace: {{ .Values.namespace }}
spec:
  labels:
    app: sloth
    role: alert-rules
    component: {{- if .Values.includeReleaseNameInMetricsLabels }} {{ .Release.Name }}-{{ .Values.appLabel }}{{- else }} {{ .Values.appLabel }}{{- end }}-latency-SLO-rules
  service: {{- if .Values.includeReleaseNameInMetricsLabels }} {{ .Release.Name }}-{{ .Values.appLabel }}{{- else }} {{ .Values.appLabel }}{{- end }}
  slos:
  - alerting:
      annotations:
        {{- if .Values.grafanaURL }}
        dashboard: {{ .Values.grafanaURL }}/d/slo-detail?var-service={{- if .Values.includeReleaseNameInMetricsLabels }}{{ .Release.Name }}-{{ .Values.appLabel }}{{- else }}{{ .Values.appLabel }}{{- end }}
        {{- end }}
        summary: SLO to measure response time - {{ .Values.latencySLO }}% of the time requests should
          be succesfully served in < {{ .Values.latency }}ms. When you receive this alert it means that
          the SLO is at risk as your error budget is getting exhausted. To know more
          about ErrorBudgets and SLOs read https://sre.google/workbook/implementing-slos/
        description: SLO to measure response time - {{ .Values.latencySLO }}% of the time requests should
          be succesfully served in < {{ .Values.latency }}ms. When you receive this alert it means that
          the SLO is at risk as your error budget is getting exhausted. To know more
          about ErrorBudgets and SLOs read https://sre.google/workbook/implementing-slos/
      name: {{- if .Values.includeReleaseNameInMetricsLabels }} {{ .Release.Name }}-{{ .Values.appLabel }}{{- else }} {{ .Values.appLabel }}{{- end }} - latency SLO is at RISK
      pageAlert:
        labels:
          alert_type: symptom
          severity: critical
      ticketAlert:
        labels:
          alert_type: symptom
          severity: warning
    description: SLO to measure response time - {{ .Values.latencySLO }}% of the time requests should
          be succesfully served in < {{ .Values.latency }}ms. When you receive this alert it means that the
      SLO is at risk as your error budget is getting exhausted. To know more about
      ErrorBudgets and SLOs read https://sre.google/workbook/implementing-slos/
    name: latency-{{- if .Values.includeReleaseNameInMetricsLabels }}{{ .Release.Name }}-{{ .Values.appLabel }}{{- else }}{{ .Values.appLabel }}{{- end }}
    objective: {{ .Values.latencySLO }}
    sli:
      events:
        {{- if .Values.includeReleaseNameInMetricsLabels }}
        errorQuery: (sum(rate(istio_request_duration_milliseconds_bucket{app="{{ .Release.Name }}-{{ .Values.appLabel}}", destination_app=~"{{ .Release.Name }}-{{ .Values.appLabel}}", le="+Inf"}[{{ printf "{{.window}}" }}])) - sum(rate(istio_request_duration_milliseconds_bucket{app="{{ .Release.Name }}-{{ .Values.appLabel}}", destination_app=~"{{ .Release.Name }}-{{ .Values.appLabel}}", le="{{ .Values.latency }}"}[{{ printf "{{.window}}" }}])))
        totalQuery: sum(rate(istio_request_duration_milliseconds_bucket{app="{{ .Release.Name }}-{{ .Values.appLabel}}", destination_app=~"{{ .Release.Name }}-{{ .Values.appLabel}}", le="+Inf"}[{{ printf "{{.window}}" }}]))
        {{- else }}
        errorQuery: (sum(rate(istio_request_duration_milliseconds_bucket{app="{{ .Values.appLabel }}", destination_app=~"{{ .Values.appLabel }}", le="+Inf"}[{{ printf "{{.window}}" }}])) - sum(rate(istio_request_duration_milliseconds_bucket{app="{{ .Values.appLabel }}", destination_app=~"{{ .Values.appLabel }}", le="{{ .Values.latency }}"}[{{ printf "{{.window}}" }}])))
        totalQuery: sum(rate(istio_request_duration_milliseconds_bucket{app="{{ .Values.appLabel }}", destination_app=~"{{ .Values.appLabel }}", le="+Inf"}[{{ printf "{{.window}}" }}]))
        {{- end }}
{{- end }}
{{- end }}
---
{{- if .Values.enabled }}
{{- if .Values.nginxIngressMetrics.enabled }}
apiVersion: sloth.slok.dev/v1
kind: PrometheusServiceLevel
metadata:
  labels:
    release: {{ .Values.prometheusReleaseLabel }}
  name: {{ .Values.appLabel }}-ingress-availability-slo
  namespace: {{ .Values.namespace }}
spec:
  labels:
    app: sloth
    role: alert-rules
    component: {{ .Values.appLabel }}-ingress-availability-SLO-rules
  service: {{ .Values.appLabel }}-ingress
  slos:
  - alerting:
      annotations:
        {{- if .Values.grafanaURL }}
        dashboard: {{ .Values.grafanaURL }}/d/slo-detail?var-service={{ .Values.appLabel }}
        {{- end }}
        summary: SLO to measure success vs errors - {{ .Values.availabilitySLO }}% of the time requests should
          be succesfully served (non 5xx). When you receive this alert it means that
          the SLO is at risk as your error budget is getting exhausted. To know more
          about ErrorBudgets and SLOs read https://sre.google/workbook/implementing-slos/
        description: SLO to measure success vs errors - {{ .Values.availabilitySLO }}% of the time requests should
          be succesfully served (non 5xx). When you receive this alert it means that
          the SLO is at risk as your error budget is getting exhausted. To know more
          about ErrorBudgets and SLOs read https://sre.google/workbook/implementing-slos/
      name: {{ .Values.appLabel }}-ingress - availability SLO is at RISK
      pageAlert:
        labels:
          alert_type: symptom
          severity: critical
      ticketAlert:
        labels:
          alert_type: symptom
          severity: warning
    description: SLO to measure success vs errors - {{ .Values.availabilitySLO }}% of the time requests should
      be succesfully served (non 5xx). When you receive this alert it means that the
      SLO is at risk as your error budget is getting exhausted. To know more about
      ErrorBudgets and SLOs read https://sre.google/workbook/implementing-slos/
    name: availability-{{ .Values.appLabel }}-ingress
    objective: {{ .Values.availabilitySLO }}
    sli:
      events:
        errorQuery: sum(rate(nginx_ingress_controller_requests{ingress=~"{{ .Values.nginxIngressMetrics.ingressName }}",status=~"5.."}[{{ printf "{{.window}}" }}]))
        totalQuery: sum(rate(nginx_ingress_controller_requests{ingress=~"{{ .Values.nginxIngressMetrics.ingressName }}"}[{{ printf "{{.window}}" }}]))
---
apiVersion: sloth.slok.dev/v1
kind: PrometheusServiceLevel
metadata:
  labels:
    release: {{ .Values.prometheusReleaseLabel }}
  name: {{ .Values.appLabel }}-ingress-latency-slo
  namespace: {{ .Values.namespace }}
spec:
  labels:
    app: sloth
    role: alert-rules
    component: {{ .Values.appLabel }}-ingress-latency-SLO-rules
  service: {{ .Values.appLabel }}-ingress
  slos:
  - alerting:
      annotations:
        {{- if .Values.grafanaURL }}
        dashboard: {{ .Values.grafanaURL }}/d/slo-detail?var-service={{ .Values.appLabel }}
        {{- end }}
        summary: SLO to measure response time - {{ .Values.latencySLO }}% of the time requests should
          be succesfully served in < 1s. When you receive this alert it means that
          the SLO is at risk as your error budget is getting exhausted. To know more
          about ErrorBudgets and SLOs read https://sre.google/workbook/implementing-slos/
        description: SLO to measure response time - {{ .Values.latencySLO }}% of the time requests should
          be succesfully served in < 1s. When you receive this alert it means that
          the SLO is at risk as your error budget is getting exhausted. To know more
          about ErrorBudgets and SLOs read https://sre.google/workbook/implementing-slos/
      name: {{ .Values.appLabel }}-ingress - latency SLO is at RISK
      pageAlert:
        labels:
          alert_type: symptom
          severity: critical
      ticketAlert:
        labels:
          alert_type: symptom
          severity: warning
    description: SLO to measure response time - {{ .Values.latencySLO }}% of the time requests should
          be succesfully served in < 1s.. When you receive this alert it means that the
      SLO is at risk as your error budget is getting exhausted. To know more about
      ErrorBudgets and SLOs read https://sre.google/workbook/implementing-slos/
    name: latency-{{ .Values.appLabel }}-ingress
    objective: {{ .Values.latencySLO }}
    sli:
      events:
        errorQuery: (sum(rate(nginx_ingress_controller_request_duration_seconds_bucket{ingress=~"{{ .Values.nginxIngressMetrics.ingressName }}",le="+Inf"}[{{ printf "{{.window}}" }}])) - sum(rate(nginx_ingress_controller_request_duration_seconds_bucket{ingress=~"{{ .Values.nginxIngressMetrics.ingressName }}",le="1"}[{{ printf "{{.window}}" }}])))
        totalQuery: sum(rate(nginx_ingress_controller_request_duration_seconds_bucket{ingress=~"{{ .Values.nginxIngressMetrics.ingressName }}",le="+Inf"}[{{ printf "{{.window}}" }}]))
{{- end }}
{{- end }}